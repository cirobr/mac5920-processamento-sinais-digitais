{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MAC0317/MAC5920\n",
    "## Introdução ao Processamento de Sinais Digitais\n",
    "### Seção 1.8: Espaços com produto interno e Ortogonalidade\n",
    "### Objetivo dessa seção: formalizar as ferramentas matemáticas necessárias à construção das transformadas de sinais (Fourier, Cosseno, Wavelets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Seção 1.8.1: Produtos internos e Normas\n",
    "\n",
    "### Definição 1.3 (produto interno):\n",
    "\n",
    "Um *produto interno* sobre um espaço vetorial $V$ é uma função $(\\cdot,\\cdot)$ de $V\\times V$ em $\\mathbb{C}$ (ou $\\mathbb{R}$) que satisfaz as propriedades abaixo $\\forall u,v,w\\in V$ e $\\forall\\alpha,\\beta\\in\\mathbb{C}$ (ou $\\mathbb{R}$):\n",
    "\n",
    "1. $(v,w) = \\overline{(w,v)}\\ \\mbox{ (simetria conjugada)}$\n",
    "2. $(\\alpha u+\\beta v,w) = \\alpha(u,w)+\\beta(v,w)\\ \\mbox{(linearidade no 1° argumento)}$\n",
    "3. $(v,v)\\ge 0\\ \\mbox{e}\\ (v,v)=0\\iff v=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No caso de espaços vetoriais sobre $\\mathbb{R}$ a conjugação complexa não tem efeito, e a condição (1) acima fica simplesmente:\n",
    "\n",
    "1. $(v,w) = (w,v)\\ \\mbox{(simetria)}$\n",
    "\n",
    "A condição (3) faz sentido, mesmo em espaços vetoriais sobre $\\mathbb{C}$, pois $(v,v)=\\overline{(v,v)}$ da condição (1) implica em $(v,v)\\in\\mathbb{R}$ ($x=\\bar{x}\\implies \\Im(x)=-\\Im(x)\\implies\\Im(x)=0$).\n",
    "\n",
    "A linearidade no 1° argumento se estende facilmente para qualquer quantidade finita de termos: basta aplicar repetidamente a propriedade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linearidade(s) do produto interno:\n",
    "\n",
    "A condição\n",
    "\n",
    "2. $(\\alpha u+\\beta v,w) = \\alpha(u,w)+\\beta(v,w)$\n",
    "\n",
    "se estende facilmente para combinações lineares finitas no primeiro argumento:\n",
    "\n",
    "$$\\left(\\sum_{k=0}^K\\alpha_ku_k,v\\right) = \\alpha_0(u_0,v)+\\left(\\sum_{k=1}^K\\alpha_ku_k,v\\right) = \\cdots = \\sum_{k=0}^K\\alpha_k(u_k,v)$$\n",
    "\n",
    "Em relação ao segundo argumento, a propriedade de simetria conjugada implica numa \"linearidade conjugada\":\n",
    "\n",
    "$$(w,\\alpha u+\\beta v) = \\overline{(\\alpha u+\\beta v,w)} = \\overline{\\alpha(u,w)+\\beta(v,w)} = \\overline{\\alpha(u,w)}+\\overline{\\beta(v,w)} = \\overline{\\alpha}(w,u)+\\overline{\\beta}(w,v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definição 1.4 (norma):\n",
    "\n",
    "Uma *norma* em um espaço vetorial $V$ é uma função $\\|\\cdot\\|$ de $V$ em $\\mathbb{R}$ que satisfaz as propriedades abaixo $\\forall v,w\\in V$ e $\\forall\\alpha\\in\\mathbb{C}$ (ou $\\mathbb{R}$):\n",
    "\n",
    "1. $\\|v\\|\\ge 0\\mbox{ e }\\|v\\|=0\\iff v=0$\n",
    "2. $\\|\\alpha v\\| = |\\alpha|\\|v\\|$\n",
    "3. $\\|v+w\\|\\le\\|v\\|+\\|w\\|$\n",
    "\n",
    "(Exercício 1.27) Todo produto interno $(\\cdot,\\cdot)$ sobre $V$ induz uma norma correspondente através da relação:\n",
    "\n",
    "$$\\|v\\| = \\sqrt{(v,v)}$$\n",
    "\n",
    "**Não se confunda:** existem normas definidas diretamente a partir das 3 condições acima mas que não correspondem a nenhum produto interno. Por exemplo, $\\|v\\|_\\infty=\\max|v_i|$ e $\\|v\\|_1=\\sum |v_i|$ satisfazem a definição acima mas não correspondem a nenhum produto interno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observação 1.8: Distância induzida por uma norma\n",
    "\n",
    "As propriedades que caracterizam distâncias em geral são $\\forall u,v,w$\n",
    "\n",
    "1. $d(v,w)\\ge 0\\mbox{  e }d(v,w)=0\\iff v=w$\n",
    "2. $d(v,w)=d(w,v)$\n",
    "3. $d(v,w)\\le d(v,u)+d(u,w)$\n",
    "\n",
    "A partir de uma norma qualquer $\\|\\cdot\\|$ pode-se definir uma distância induzida por\n",
    "\n",
    "$$d(v,w) = \\|v-w\\|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observação 1.9: Interpretação de energia em sinais\n",
    "\n",
    "Frequentemente usamos o valor $\\|v\\|^2$ como uma medida de energia do sinal. Isso tem origem em analogias com modelos físicos (energia cinética $\\propto v^2$, energia potencial $\\propto x^2$, energia elétrica dissipada $\\propto V^2$).\n",
    "\n",
    "Em sinais, essa medida de energia tem algumas propriedades importantes, dentre as quais a **Identidade de Parseval**, que estabelece que a energia de um sinal equivale à soma das energias de suas componentes (senoidais, exponenciais)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Seção 1.8.2: Exemplos de espaços com produtos internos e normas correspondentes\n",
    "\n",
    "**Exemplo 1.12:** Em $\\mathbb{R}^N$ o produto interno mais comum (canônico) é\n",
    "\n",
    "$$(x,y) = \\sum_{i=0}^{N-1} x_iy_i = x_0y_0+x_1y_1+\\cdots+x_{N-1}y_{N-1}$$\n",
    "\n",
    "A norma correspondente é\n",
    "\n",
    "$$\\|x\\| = \\left(\\sum_{i=0}^{N-1} x_ix_i\\right)^{\\frac{1}{2}} = \\left(x_0^2+x_1^2+\\cdots+x_{N-1}^2\\right)^{\\frac{1}{2}}$$\n",
    "\n",
    "e é chamada de *Norma Euclideana*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemplo 1.13:** Em $\\mathbb{C}^N$ o produto interno canônico é\n",
    "\n",
    "$$(x,y) = \\sum_{i=0}^{N-1} x_i\\overline{y_i} = x_0\\overline{y_0}+x_1\\overline{y_1}+\\cdots+x_{N-1}\\overline{y_{N-1}}$$\n",
    "\n",
    "cuja norma correspondente é\n",
    "\n",
    "$$\\|x\\| = \\left(\\sum_{i=0}^{N-1} x_i\\overline{x_i}\\right)^{\\frac{1}{2}} = \\left(|x_0|^2+|x_1|^2+\\cdots+|x_{N-1}|^2\\right)^{\\frac{1}{2}}$$\n",
    "\n",
    "**Atenção:** tanto as conjugações complexas quanto os valores absolutos nas equações acima são importantes! Note que para qualquer $z\\in\\mathbb{C}$, considerando sua representação polar $z=Ae^{i\\varphi}$ (onde $A=|z|$ e $\\varphi=\\measuredangle z$) temos $z\\overline{z}=Ae^{i\\varphi}\\overline{Ae^{i\\varphi}}=Ae^{i\\varphi}Ae^{-i\\varphi}=A^2e^0=|z|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Verificação:\n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "1.&\\overline{(x,y)}=\\overline{\\sum_{i=0}^{N-1} x_i\\overline{y_i}}=\\sum_{i=0}^{N-1} \\overline{x_i\\overline{y_i}}=\\sum_{i=0}^{N-1} \\overline{x_i}y_i=(y,x)\\\\\n",
    "2.&(\\alpha x+\\beta y,z) = \\sum_{i=0}^{N-1} (\\alpha x_i+\\beta y_i)\\overline{z_i}=\\alpha\\sum_{i=0}^{N-1}x_i\\overline{z_i}+\\beta\\sum_{i=0}^{N-1}y_i\\overline{z_i}\\\\\n",
    "&=\\alpha(x,z)+\\beta(y,z)\\\\\n",
    "3.&(x,x) = \\sum_{i=0}^{N-1} x_i\\overline{x_i} = \\sum_{i=0}^{N-1} |x_i|^2\\ge 0\\\\\n",
    "&\\mbox{e}\\ \\ (x,x)=0\\iff \\vert x_i\\vert^2=0\\ \\forall i\\iff x=0\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemplo 1.14:** Em $\\mathcal{M}_{M,N}\\left(\\mathbb{C}\\right)=\\mathbb{C}^{M\\times N}$ o produto interno canônico é\n",
    "\n",
    "$$(A,B) = \\sum_{i=0}^{M-1}\\sum_{j=0}^{N-1} A_{ij}\\overline{B_{ij}}$$\n",
    "\n",
    "cuja norma correspondente, conhecida como *Norma de Frobenius* é\n",
    "\n",
    "$$\\|A\\| = \\left(\\sum_{i=0}^{M-1}\\sum_{j=0}^{N-1}|A_{ij}|^2\\right)^{\\frac{1}{2}}$$\n",
    "\n",
    "O espaço $\\mathbb{C}^{M\\times N}$ é essencialmente o mesmo que $\\mathbb{C}^{MN}$ (serialização de linhas ou colunas).\n",
    "\n",
    "**Observação:** pulamos os exemplos 1.15 a 1.17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Seção 1.8.3: Ortogonalidade\n",
    "\n",
    "Da geometria elementar em 2D ou 3D sabemos que\n",
    "\n",
    "$$(v,w) = \\|v\\|\\|w\\|\\cos(\\theta)$$\n",
    "\n",
    "onde $\\theta$ é o ângulo formado entre os vetores $v$ e $w$. Daí segue que se $v,w\\neq 0$ então\n",
    "$$v\\perp w\\iff\\theta=\\frac{\\pi}{2}\\iff\\cos(\\theta)=0\\iff(v,w)=0.$$\n",
    "\n",
    "Isso motiva a\n",
    "\n",
    "**Definição 1.5:** Num espaço vetorial com produto interno, dizemos que $v\\neq 0$ e $w\\neq 0$ são *ortogonais* ($v\\perp w$) se $(v,w)=0$.\n",
    "\n",
    "Um subconjunto $S\\subseteq V$ é *ortogonal por pares* (ou simplesmente *ortogonal*) se $v\\perp w,\\ \\forall v,w\\in S\\backslash\\{0\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemplo 1.18:** Seja $S=\\{e_i|i=0,\\ldots,N-1\\}\\subset\\mathbb{R}^N$ onde\n",
    "$$e_i = (0,\\ldots,0,\\overbrace{1}^{ind=i},0,\\ldots,0)\\quad\\mbox{ou equivalentemente}\\quad (e_i)_j=0\\iff i=j.$$\n",
    "\n",
    "Então $S$ é ortogonal, pois se $i\\neq j$\n",
    "$$(e_i,e_j)=\\sum_{k=0}^{N-1}(e_i)_k(e_j)_k=0,$$\n",
    "já que não podem valer $i=k$ e $j=k$ ao mesmo tempo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemplo 1.18 (cont.):** O mesmo conjunto $S=\\{e_i|i=0,\\ldots,N-1\\}$ visto como subconjunto de $\\mathbb{C}^N$ também é ortogonal, pois se $i\\neq j$:\n",
    "$$(e_i,e_j)=\\sum_{k=0}^{N-1}(e_i)_k\\overline{(e_j)_k}=\\sum_{k=0}^{N-1}(e_i)_k(e_j)_k=0,$$\n",
    "onde a penúltima igualdade vale porque $(e_j)_k\\in\\{0,1\\}$.\n",
    "\n",
    "**Observação:** pulamos o exemplo 1.19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemplo 1.20:** Seja $S=\\{E_k|k=0,\\ldots,N-1\\}$ como definidos anteriormente, onde\n",
    "$$(E_k)_n = e^{i2\\pi k\\frac{n}{N}}.$$\n",
    "\n",
    "Então $S$ é ortogonal:\n",
    "\n",
    "$$(E_k,E_l) = \\sum_{n=0}^{N-1}e^{i2\\pi k\\frac{n}{N}}\\overline{e^{i2\\pi l\\frac{n}{N}}} = \\sum_{n=0}^{N-1}e^{i2\\pi k\\frac{n}{N}}e^{-i2\\pi l\\frac{n}{N}}$$\n",
    "$$ \\phantom{(E_k,E_l)} = \\sum_{n=0}^{N-1}e^{i2\\pi (k-l)\\frac{n}{N}} = \\sum_{n=0}^{N-1}\\left(e^{i2\\pi (k-l)\\frac{1}{N}}\\right)^n = \\cdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A última equação pode ser desenvolvida lembrando da equação de soma dos termos de uma Progressão Geométrica:\n",
    "$$X = 1+z+z^2+\\cdots+z^{N-1} \\implies X = 1+zX-z^N\\implies X = \\frac{1-z^N}{1-z},$$\n",
    "\n",
    "Substituindo $z=e^{i2\\pi (k-l)\\frac{1}{N}}$ em\n",
    "$$(E_k,E_l) = \\sum_{n=0}^{N-1}\\left(e^{i2\\pi (k-l)\\frac{1}{N}}\\right)^n$$\n",
    "e usando $z\\neq 1$ (pois $0\\le k,l<N$ e $k\\neq l$) temos\n",
    "$$(E_k,E_l) = \\frac{1-\\left(e^{i2\\pi (k-l)\\frac{1}{N}}\\right)^N}{1-e^{i2\\pi (k-l)\\frac{1}{N}}} = \\frac{1-e^{i2\\pi (k-l)}}{1-e^{i2\\pi (k-l)\\frac{1}{N}}}=0$$\n",
    "(a última equação segue de $e^{i2\\pi (k-l)}=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Comentário 1.10:** Usando essencialmente o mesmo argumento pode-se provar que a mesma ortogonalidade vale entre as matrizes básicas $E_{k,l}$, ou seja, dados $k,l,r,s$ com $k\\neq r$ ou $l\\neq s$, então $(E_{k,l},E_{r,s})=0$ usando o produto interno do Exemplo 1.14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Seção 1.8.4: Desigualdade de Cauchy-Schwarz\n",
    "\n",
    "### Teorema 1.1 (Cauchy-Schwarz)\n",
    "\n",
    "Dados $v$ e $w$ em um espaço vetorial $V$, temos\n",
    "$$|(v,w)| \\le \\|v\\|\\|w\\|,$$\n",
    "onde a norma do lado direito é aquela induzida pelo produto interno do lado esquerdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plotCauchySchwarz():\n",
    "    plt.axis('off')\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.arrow(0, 0, 0.53, 0.53, head_width=0.05, head_length=0.1, fc='k', ec='k');\n",
    "    plt.annotate(\"$v$\",(0.6,0.6),size=20)\n",
    "    plt.arrow(0, 0, 1.5, 0, head_width=0.05, head_length=0.1, fc='k', ec='k');\n",
    "    plt.annotate(\"$w$\",(1.6,0),size=20)\n",
    "    plt.arrow(0, 0, 0.56, 0, head_width=0.03, head_length=0.05, fc='k', ec='k');\n",
    "    plt.annotate(\"$cw$\",(0.6,0.05),size=20)\n",
    "    plt.arrow(0.6, 0.6, 0, -0.6, ls=':', fc='k', ec='k')\n",
    "    plt.arrow(0.6, 0.1, -0.1, 0, ls=':', fc='k', ec='k')\n",
    "    plt.arrow(0.5, 0.1, 0, -0.1, ls=':', fc='k', ec='k')\n",
    "    plt.annotate(\"$\\|v-cw\\|^2≥0$\",(0.7,0.3),size=20)\n",
    "    plt.annotate(\"Esquema da demonstração da desigualdade de Cauchy-Schwarz:\\nprojeta-se o vetor $v$ ortogonalmente sobre o vetor $w$, e\\nconsidera-se o segmento que une $v$ a sua projeção $cw$.\",(1.5,0.3),size=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotCauchySchwarz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prova do teorema 1.1\n",
    "\n",
    "Essencialmente ela parte da observação de que a *distância* entre um vetor $v$ e sua projeção ortogonal $cw$ sobre $w$ (aqui $c$ é um escalar) é sempre $\\ge 0$, ou equivalentemente, $\\|v-cw\\|^2\\ge 0$.\n",
    "\n",
    "Note que\n",
    "$$\\begin{array}{ll}\n",
    "\\|v-cw\\|^2&=(v-cw,v-cw)=(v,v)-c(w,v)-\\bar{c}(v,w)+c\\bar{c}(w,w)\\\\\n",
    "    &=\\|v\\|^2+|c|^2\\|w\\|^2-(c\\overline{(v,w)}+\\bar{c}(v,w)).\\\\\n",
    "    &\\ge 0\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Agora, pela ortogonalidade da projeção, $(v-cw,w)=0$, ou seja,\n",
    "$$c=\\frac{(v,w)}{\\|w\\|^2}.$$\n",
    "\n",
    "Substituindo este $c$ na equação anterior,\n",
    "$$\\begin{array}{ll}\n",
    "\\|v\\|^2&+\\frac{|(v,w)|^2}{\\|w\\|^4}\\|w\\|^2-\\left(\\frac{(v,w)}{\\|w\\|^2}(w,v)+\\frac{\\overline{(v,w)}}{\\|w\\|^2}(v,w)\\right)\\\\\n",
    "&=\\|v\\|^2+\\frac{|(v,w)|^2}{\\|w\\|^2}-2\\frac{|(v,w)|^2}{\\|w\\|^2}\\\\\n",
    "&=\\|v\\|^2-\\frac{|(v,w)|^2}{\\|w\\|^2}\\ge 0,\\end{array}$$\n",
    "de onde sai a equação de Cauchy-Schwarz $|(v,w)| \\le \\|v\\|\\|w\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Seção 1.8.5: Bases e Decomposição Ortogonal\n",
    "\n",
    "Vimos no Exemplo 1.18 como o conjunto $S=\\{e_i|i=0,\\ldots,N-1\\}$ onde\n",
    "$$e_i = (0,\\ldots,0,\\overbrace{1}^{ind=i},0,\\ldots,0)\\quad\\mbox{ou equivalentemente}\\quad (e_i)_j=0\\iff i=j$$\n",
    "era ortogonal. Esse conjunto é chamado de base canônica e serve tanto para $\\mathbb{R}^N$ quanto $\\mathbb{C}^N$, pois qualquer vetor $x=(x_0,x_1,\\ldots,x_{N-1})$ de um desses espaços pode ser escrito como combinação linear dos elementos de $S$, mais especificamente,\n",
    "$$x = \\sum_{i=0}^{N-1}x_ie_i.$$\n",
    "\n",
    "Assim $S$ é um conjunto que permite a construção de qualquer elemento de $\\mathbb{R}^N$ ou $\\mathbb{C}^N$ como combinações lineares de $S$, e é minimal pois nenhum conjunto com menos de $N$ vetores tem essa propriedade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Definição 1.6 (conjunto gerador)\n",
    "\n",
    "Dizemos que um subconjunto $S\\subset V$ (finito ou infinito) em um espaço vetorial $V$ é um *conjunto gerador* de $V$ se qualquer $x\\in V$ pode ser representado como uma combinação linear finita de elementos de $S$, ou seja, se existem $K\\in\\mathbb{N}$, escalares $a_i,\\ i=0,\\ldots,K-1$ e vetores $v_i\\in S,\\ i=0,\\ldots,K-1$ tais que\n",
    "$$x = \\sum_{i=0}^{K-1}a_iv_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Definição 1.7 (conjunto linearmente independente)\n",
    "\n",
    "Dizemos que um subconjunto $S\\subset V$ (finito ou infinito) em um espaço vetorial $V$ é um *conjunto linearmente independente* se para qualquer subconjunto finito $\\{v_i,\\ i=0,\\ldots,K-1\\}\\subseteq S$, a única solução do sistema homogêneo\n",
    "$$\\sum_{i=0}^{K-1}a_iv_i=0$$\n",
    "é a solução trivial $a_0=a_1=\\cdots=a_{K-1}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Definição 1.8 (base)\n",
    "\n",
    "Dizemos que um subconjunto $S\\subset V$ (finito ou infinito) é uma *base* de $V$ se $S$ é um conjunto gerador e linearmente independente.\n",
    "\n",
    "*Observação:* ainda que $S$ possa ter infinitos elementos, a definição acima exige que todos os elementos de $V$ possam ser representados como combinações lineares *finitas*, ou seja, usando subconjuntos finitos de $S$. O nome técnico para isso é *Base de Hamel*, e exclui por exemplos conjuntos como a extensão da base canônica para sinais infinitos ou bi-infinitos ($e_i=(\\ldots,0,1,0,\\ldots)$), que demandariam combinações lineares infinitas (séries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemplo 1.21:** No espaço $\\mathcal{M}_{m,n}(\\mathbb{C})=\\mathbb{C}^{m\\times n}$ de matrizes complexas $m\\times n$, considere as $mn$ matrizes $A_{i,j}$ definidas por\n",
    "$$\\left(A_{p,q}\\right)_{i,j} = 1\\ \\mbox{se}\\ p,q=i,j,\\ \\mbox{e}\\ 0\\ \\mbox{caso contrário}.$$\n",
    "Então o conjunto $S=\\{A_{p,q}\\mid 0\\le p\\le m-1,\\ 0\\le q\\le n-1\\}$\n",
    "forma uma base de $\\mathcal{M}_{m,n}$, já que qualquer matriz $X\\in\\mathcal{M}_{m,n}$ satisfaz\n",
    "$$X=\\sum_{i=0}^{m-1}\\sum_{j=0}^{n-1}x_{i,j}A_{i,j}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemplo 1.22:** Seja $P$ o conjunto de todas os polinômios de grau $n$, ou seja, funções da forma\n",
    "$$p(x)=a_0+a_1x+\\cdots+a_nx^n.$$\n",
    "Então $P$ é um espaço vetorial (usando a soma de polinômios e o produto por escalar usuais -- fica como exercício) e o conjunto infinito\n",
    "$$S=\\{1,x,\\ldots,x^n,\\ldots\\}$$\n",
    "é uma base para $P$. De fato $S$ gera $P$ por construção, e o conjunto $S$ é linearmente independente, pois a única maneira de uma equação da forma\n",
    "$$a_0+a_1x+\\cdots+a_nx^n=0,\\ \\forall x$$\n",
    "valer é quando $a_i=0,\\ \\forall i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Base e dimensão\n",
    "\n",
    "Se $S$ é uma base finita para $V$, o tamanho $N=|S|$ de $S$ é denominado **dimensão** de $V$, e é possível provar que qualquer outra base de $V$ precisa possuir exatamente $N$ elementos. Mais ainda, em um espaço $N$ dimensional, qualquer conjunto de $N$ vetores linearmente independentes é necessariamente uma base.\n",
    "\n",
    "Pelos exemplos acima podemos estabelecer que tanto $\\mathbb{R}^N$ quanto $\\mathbb{C}^N$ são espaços $N$-dimensionais, e que tanto $\\mathcal{M}_{m,n}(\\mathbb{R})$ quanto $\\mathcal{M}_{m,n}(\\mathbb{C})$ são espaços $mn$-dimensionais. Por outro lado, o espaço $P$ dos polinômios possui dimensão infinita.\n",
    "\n",
    "*Não se confunda:* apesar de representarmos o conjunto $\\mathbb{C}$ dos complexos em um plano, como espaço vetorial $\\mathbb{C}$ tem dimensão 1, pois qualquer elemento de $\\mathbb{C}$ pode ser escrito como $\\alpha\\cdot(1)$, onde $\\alpha\\in\\mathbb{C}$; pela mesma razão, os $N$ elementos da base canônica de $\\mathbb{C}^N$ são suficientes para produzir qualquer vetor com $N$ componentes complexas, pois os *escalares* nesse caso são eles próprios complexos, e então a dimensão de $\\mathbb{C}^N$ é de fato $N$ (e não $2N$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bases ortogonais e ortonormais\n",
    "\n",
    "Em um espaço $N$ dimensional, qualquer conjunto ortogonal de $N$ vetores não-nulos é necessariamente uma base. Isso decorre do seguinte fato:\n",
    "\n",
    "#### Teorema 1.2: Se $S\\subset V\\backslash\\{0\\}$ é ortogonal, então $S$ é linearmente independente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Prova:** seja $\\{v_0,v_1,\\ldots,v_{N-1}\\}\\subset S$ com $v_i\\neq 0,\\ \\forall i$ e $(v_i,v_j)=0,\\ \\forall v_i,v_j\\in S,\\ i\\neq j$; então o sistema\n",
    "$\\displaystyle\\sum_{i=0}^{K-1}a_iv_i=0$ pode ser resolvido fazendo produtos internos com cada $v_j$:\n",
    "$$0=\\displaystyle\\left(\\sum_{i=0}^{K-1}a_iv_i,v_j\\right) = \\sum_{i=0}^{K-1}a_i(v_i,v_j) = a_j(v_j,v_j),$$\n",
    "onde o somatório desaparece porque $(v_i,v_j)=0$ para todo $i\\neq j$. Pela última equação, como $(v_j,v_j)=\\|v_j\\|^2\\neq 0$ segue que $a_j=0$ (esse argumento vale $\\forall j$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mudanças de representação para bases ortogonais\n",
    "\n",
    "Se $S$ é uma base qualquer para $V$, temos uma garantia de que é possível representar qualquer $x\\in V$ como $x=\\displaystyle\\sum_{i=0}^{N-1}a_iv_i$ (Eq. I).\n",
    "\n",
    "Uma pergunta razoável é: \"como encontrar os $a_i$'s?\".\n",
    "\n",
    "Como veremos, existe uma solução muito simples quando $S$ é uma base ortogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Teorema 1.3: Se $S=\\{v_0,v_1,\\ldots,v_{N-1}\\}$ é uma base ortogonal para $V$, então qualquer $x\\in V$ pode ser escrito como\n",
    "$$x=\\displaystyle\\sum_{i=0}^{N-1}a_iv_i$$\n",
    "onde\n",
    "$$a_j=\\frac{(x,v_j)}{(v_j,v_j)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Prova:** podemos proceder outra vez por eliminação, fazendo  produtos internos da Eq. I com cada $v_j$:\n",
    "$$(x,v_j) = \\left(\\sum_{i=0}^{N-1}a_iv_i,v_j\\right) = \\sum_{i=0}^{N-1}a_i(v_i,v_j) = a_j(v_j,v_j),$$\n",
    "de onde segue que\n",
    "$$a_j=\\frac{(x,v_j)}{(v_j,v_j)}=\\frac{(x,v_j)}{\\|v_j\\|^2}.$$\n",
    "\n",
    "Observe que como $S$ é base, $0\\not\\in S$ e portanto $(v_j,v_j)=\\|v_j\\|^2\\neq 0,\\ \\forall v_j\\in S$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Observe** que a expressão\n",
    "$$a_j=\\frac{(x,v_j)}{(v_j,v_j)}=\\frac{(x,v_j)}{\\|v_j\\|^2}$$\n",
    "já apareceu na demonstração de Cauchy-Schwarz, como a constante associada à projeção ortogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plotCoefProjeção():\n",
    "    plt.axis('off')\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.arrow(0, 0, 0.53, 0.53, head_width=0.05, head_length=0.1, fc='k', ec='k');\n",
    "    plt.annotate(\"$x$\",(0.6,0.6),size=20)\n",
    "    plt.arrow(0, 0, 1.5, 0, head_width=0.05, head_length=0.1, fc='k', ec='k');\n",
    "    plt.annotate(\"$v_j$\",(1.6,0),size=20)\n",
    "    plt.arrow(0, 0, 0.56, 0, head_width=0.03, head_length=0.05, fc='k', ec='k');\n",
    "    plt.annotate(\"$a_jv_j$\",(0.6,0.05),size=20)\n",
    "    plt.arrow(0.6, 0.6, 0, -0.6, ls=':', fc='k', ec='k')\n",
    "    plt.arrow(0.6, 0.1, -0.1, 0, ls=':', fc='k', ec='k')\n",
    "    plt.arrow(0.5, 0.1, 0, -0.1, ls=':', fc='k', ec='k')\n",
    "    plt.annotate(\"$a_j=\\\\frac{(x,v_j)}{(v_j,v_j)}=\\\\frac{(x,v_j)}{||v_j||^2}$\",(1.0,0.3),size=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotCoefProjeção()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bases ortonormais\n",
    "\n",
    "#### Definição 1.9: Um conjunto ortogonal $S$ é dito ortonormal se $\\|v\\|=1,\\ \\forall v\\in S$.\n",
    "\n",
    "**Observação 1.11:** É sempre possível normalizar um conjunto ortogonal $S$ para obter um conjunto ortonormal $S'$ substituindo cada $v\\in S$ por $$v'=\\frac{v}{\\|v\\|}.$$\n",
    "Isso garante que $\\|v'\\| = \\left\\|\\frac{v}{\\|v\\|}\\right\\| = \\frac{\\|v\\|}{\\|v\\|} = 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No caso de uma base ortonormal $S$, os coeficientes $a_i$ da representação de $x=\\displaystyle\\sum_{i=0}^{N-1}a_iv_i$ podem ser obtidos como\n",
    "$$a_i=\\frac{(x,v_k)}{(v_k,v_k)}=\\frac{(x,v_k)}{1}=(x,v_k).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemplo 1.23:** Considere o conjunto $S=\\{E_k| k=0,\\ldots,N-1\\}\\subset\\mathbb{C}^N$. No Exemplo 1.20 vimos que eles são ortogonais, logo linearmente independentes, e como $|S|=N$, segue que $S$ é uma base de $\\mathbb{C}^N$.\n",
    "\n",
    "Pelo teorema 1.8.3, segue que qualquer $x\\in\\mathbb{C}^N$ pode ser escrito como $x=\\displaystyle\\sum_{k=0}^{N-1}a_kE_k$ com $a_k=\\frac{(x,E_k)}{(E_k,E_k)}=\\frac{(x,E_k)}{N};$\n",
    "\n",
    "o último passo segue de\n",
    "\n",
    "$$(E_k,E_k)=\\sum_{i=0}^{N-1}E_{k,j}\\overline{E_{k,j}}=\\sum_{i=0}^{N-1}e^0=N.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Isso mostra que qualquer $x\\in\\mathbb{C}^N$ pode ser escrito como:\n",
    "$$x=\\frac{1}{N}\\displaystyle\\sum_{k=0}^{N-1}(x,E_k)E_k\\quad\\mbox{onde}\\quad(x,E_k) = \\sum_{n=0}^{N-1}x_ne^{-i2\\pi kn/N}.$$\n",
    "Essa equação é a conhecida *Transformada Discreta de Fourier (DFT)*, que será estudada em profundidade nos capítulos 2 e 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plotProjeçãoDFT():\n",
    "    plt.axis('off')\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.arrow(0, 0, 0.97, 0.55, head_width=0.05, head_length=0.1, fc='k', ec='k');\n",
    "    plt.annotate(\"$x$\",(1.1,0.6),size=20)\n",
    "    plt.arrow(0, 0, 1.5, 0, head_width=0.05, head_length=0.1, fc='k', ec='k');\n",
    "    plt.annotate(\"$E_j$\",(1.6,0),size=20)\n",
    "    plt.arrow(0, 0, 1.03, 0, head_width=0.03, head_length=0.05, fc='k', ec='k');\n",
    "    plt.annotate(\"$c_jE_j$\",(1.1,0.1),size=20)\n",
    "    plt.arrow(1.06, 0.6, 0, -0.6, ls=':', fc='k', ec='k')\n",
    "    plt.arrow(1.06, 0.1, -0.1, 0, ls=':', fc='k', ec='k')\n",
    "    plt.arrow(0.96, 0.1, 0, -0.1, ls=':', fc='k', ec='k')\n",
    "    plt.arrow(0, 0, 0, 1.5, head_width=0.05, head_length=0.1, fc='k', ec='k');\n",
    "    plt.annotate(\"$E_l$\",(0.05,1.5),size=20)\n",
    "    plt.arrow(0, 0, 0, 0.56, head_width=0.03, head_length=0.05, fc='k', ec='k');\n",
    "    plt.arrow(0, 0.7, 0, 0, ls=':', fc='k', ec='k')\n",
    "    plt.annotate(\"$c_lE_l$\",(0.05,0.65),size=20)\n",
    "    plt.arrow(1.06, 0.6, -1.06, 0, ls=':', fc='k', ec='k')\n",
    "    plt.arrow(0.1, 0.6, 0, -0.1, ls=':', fc='k', ec='k')\n",
    "    plt.arrow(0.1, 0.5, -0.1, 0, ls=':', fc='k', ec='k')\n",
    "    plt.annotate(\"$x=c_jE_j+c_lE_l\\\\quad onde\\\\quad c_k=\\\\frac{(x,E_k)}{(E_k,E_k)}=\\\\frac{(x,E_k)}{N}$\",(1.0,1.0),size=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotProjeçãoDFT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemplo 1.24:** Usando exatamente o mesmo argumento do exemplo anterior, verificamos que qualquer matriz $A\\in\\mathcal{M}_{m,n}(\\mathbb{C})$ pode ser escrita como\n",
    "$$A = \\sum_{k=0}^{m-1}\\sum_{l=0}^{n-1}\\frac{(A,E_{k,l})}{(E_{k,l},E_{k,l})}E_{k,l} = \\frac{1}{mn}\\sum_{k=0}^{m-1}\\sum_{l=0}^{n-1}(A,E_{k,l})E_{k,l},$$\n",
    "onde\n",
    "$$(A,E_{k,l}) = \\sum_{r=0}^{m-1}\\sum_{s=0}^{n-1} A_{r,s}e^{-i2\\pi(kr/m+ls/n)}$$\n",
    "e usamos o fato de que $(E_{k,l},E_{k,l})=mn$ (Exercício 1.31).\n",
    "\n",
    "Essa equação é a versão 2D da *Transformada Discreta de Fourier (DFT2D)*, que também receberá maior atenção nos capítulos 2 e 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identidade de Parseval\n",
    "\n",
    "Quando $S=\\{v_0,v_1,\\ldots,v_{N-1}\\}$ é uma base ortonormal para $V$ (ou seja, $\\|v_j\\|=1,\\ \\forall j$), temos que qualquer $x\\in V$ pode ser escrito como $x=\\displaystyle\\sum_j a_jv_j$ com $a_j=(x,v_j)$, e além disso\n",
    "$$\\begin{array}{ll}\\|x\\|^2 &= (x,x) = \\left(\\displaystyle\\sum_j a_jv_j,\\sum_k a_kv_k\\right)\\\\\n",
    "&=\\displaystyle\\sum_j\\sum_k a_j\\overline{a_k}(v_j,v_k)=\\sum_j a_j\\overline{a_j}=\\sum_j|a_j|^2,\\end{array}$$\n",
    "onde a penúltima equação segue do fato que $(v_j,v_k)=0$ se $j\\neq k$ e $(v_j,v_j)=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Essa é a conhecida *Identidade de Parseval*, que estabelece que se $a=(\\ldots,a_j,\\ldots)$ são os coeficientes de $x$ em uma base ortonormal qualquer, então\n",
    "$$\\|x\\|^2=\\sum_j|x_j|^2=\\sum_j|a_j|^2=\\|a\\|^2,$$\n",
    "mostrando que o conceito de *energia*, associado à quantidade $\\|x\\|^2$, não depende da base ortonormal utilizada para representar os vetores."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
